{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xet-u24TZc-"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers accelerate evaluate rouge_score\n",
        "!pip install transformers accelerate evaluate rouge_score\n",
        "!pip install sacrebleu\n",
        "!pip install -U ray\n",
        "!pip install bert_score\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG1DYU8YH50R"
      },
      "outputs": [],
      "source": [
        "# pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "banvXjpoDZ6Q"
      },
      "outputs": [],
      "source": [
        "# import wandb\n",
        "\n",
        "# wandb.init(project=\"Conversation\")\n",
        "# # 7ae06a5af942cfd2e21de64ee697126eb8b0d0b9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNL3nT2CkTAX",
        "outputId": "4792b12f-e7d6-41ad-b3d4-4b396a12f837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT2n1WgTnN1U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import LineByLineTextDataset\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ4s0S19nlH2"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_json('/content/drive/MyDrive/AESLC-master/conversation-qa/train.jsonl', lines=True)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcPFh6ScWekr"
      },
      "outputs": [],
      "source": [
        "df_dev = pd.read_json('/content/drive/MyDrive/AESLC-master/conversation-qa/dev.jsonl', lines=True)\n",
        "df_dev.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELPkOTUDW4-Y"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_json('/content/drive/MyDrive/AESLC-master/conversation-qa/test.jsonl', lines=True)\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMeaR0gBpTwX"
      },
      "outputs": [],
      "source": [
        "prompt_word_count_list = []\n",
        "for sentence in df_train['prompt']:\n",
        "  prompt_word_count_list.append(len(sentence.split(\" \")))\n",
        "\n",
        "response_word_count_list = []\n",
        "for sentence in df_train['response']:\n",
        "  response_word_count_list.append(len(sentence.split(\" \")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNMOTCP2rd-t"
      },
      "outputs": [],
      "source": [
        "plt.hist(prompt_word_count_list, bins=10, color='red')\n",
        "plt.title('Prompt word count')\n",
        "plt.xlabel('Prompt')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STjA2igOsvDR"
      },
      "outputs": [],
      "source": [
        "plt.hist(response_word_count_list, bins=10, color='red')\n",
        "plt.title('Response word count')\n",
        "plt.xlabel('Response')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ywok4sDU41E"
      },
      "outputs": [],
      "source": [
        "with open('/content/train.txt', 'w') as file:\n",
        "    for prompt, response in zip(df_train['prompt'], df_train['response']):\n",
        "      file.write(\"Prompt: \" + prompt + \" Response: \" + response + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diJeHnIwWv3E"
      },
      "outputs": [],
      "source": [
        "with open('/content/dev.txt', 'w') as file:\n",
        "    for prompt, response in zip(df_dev['prompt'], df_dev['response']):\n",
        "      file.write(\"Prompt: \" + prompt + \" Response: \" + response + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiDeWkk8XB6k"
      },
      "outputs": [],
      "source": [
        "# with open('/content/test.txt', 'w') as file:\n",
        "#     for prompt, response in zip(df_test['prompt'], df_test['response']):\n",
        "#       file.write(\"Prompt: \" + prompt + \" Response: \" + response + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4jAZyoKbEyo"
      },
      "outputs": [],
      "source": [
        "train_file_path = \"/content/drive/MyDrive/convertational/train.txt\"\n",
        "eval_file_path = \"/content/drive/MyDrive/convertational/dev.txt\"\n",
        "model_name = 'gpt2'\n",
        "rouge = evaluate.load('rouge')\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "#meteor = evaluate.load('meteor')\n",
        "output_dir = '/content/output'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 10\n",
        "save_steps = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YT0Qa18s2mo",
        "outputId": "8e38e232-b91f-4d4d-a2a1-5e891b7f1407"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3750"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_dev['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuYF81w_z6HN"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Your code that may generate warnings\n",
        "\n",
        "# To reset the warning settings to their defaults:\n",
        "warnings.resetwarnings()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fVPEeNupbiLx"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path, tokenizer):\n",
        "    dataset = LineByLineTextDataset(\n",
        "                tokenizer=tokenizer,\n",
        "                file_path=file_path,\n",
        "                block_size=512\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm\n",
        "    )\n",
        "\n",
        "    return data_collator\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    # TODO: Separate only the subject from string\n",
        "    # Ensure that for preds, you have a list of only the generated subject parts\n",
        "    # For labels, it should be a list of list of only the reference subjects\n",
        "    # NO OTHER CONTENT: EMAIL / SEPARATORS SHOULD BE OUTPUT AFTER POSTPROCESSING\n",
        "\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    \"\"\"\n",
        "    Original Trainer may have a memory leak.\n",
        "    This is a workaround to avoid storing too many tensors that are not needed.\n",
        "    \"\"\"\n",
        "    # print('logits:', logits.shape)\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    # print('pred_ids:', pred_ids.shape)\n",
        "\n",
        "    return pred_ids, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    i = 0\n",
        "    size = len(df_dev['prompt'])\n",
        "    list_of_prediction = []\n",
        "    while i < size:\n",
        "              print(i)\n",
        "#               print(list_of_references[i])\n",
        "              inputs = tokenizer(\"Prompt: \" + df_dev['prompt'].iloc[i] + ' Response: ', return_tensors=\"pt\")\n",
        "              inputs['input_ids'] = inputs['input_ids'].cpu()  # Move input tensor to CPU if necessary\n",
        "              device = torch.device(\"cuda:0\")  # Specify the CUDA device\n",
        "              model.to(device)  # Move the model to the CUDA device\n",
        "\n",
        "              # Move the input tensor to the CUDA device\n",
        "              inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "              outputs = model.generate(inputs['input_ids'], max_new_tokens=15, do_sample=True, top_k=30, top_p=0.95)\n",
        "              prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "              # Generate outputs using the model on the CUDA device\n",
        "              #print(prediction)\n",
        "              prediction = prediction.split('Response: ')[1]\n",
        "              list_of_prediction.append(prediction)\n",
        "#               print(prediction)\n",
        "              i = i + 1\n",
        "    result = rouge.compute(predictions=list_of_prediction, references=df_dev['response'])\n",
        "    results_sacrebleu = sacrebleu.compute(predictions=list_of_prediction, references=df_dev['response'], lowercase = True)\n",
        "    results_bert = bertscore.compute(predictions=list_of_prediction, references=df_dev['response'], lang=\"en\")\n",
        "    #results_meteor = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # wandb.log({\n",
        "    #     \"R1\": round(result[\"rouge1\"], 4),\n",
        "    #     \"R2\": round(result[\"rouge2\"], 4),\n",
        "    #     \"RL\": round(result[\"rougeL\"], 4),\n",
        "    #     \"RLsum\": round(result[\"rougeLsum\"], 4),\n",
        "    #     \"bleu\": round(results_sacrebleu[\"score\"], 4),\n",
        "    #     \"precision1\":round(results_bert[\"precision\"][0], 4),\n",
        "    #     \"precision2\":round(results_bert[\"precision\"][1], 4),\n",
        "    #     \"recall1\":round(results_bert[\"recall\"][0], 4),\n",
        "    #     \"recall2\":round(results_bert[\"recall\"][1], 4),\n",
        "    #     \"f1-score1\":round(results_bert[\"f1\"][0], 4),\n",
        "    #     \"f1-score2\":round(results_bert[\"f1\"][1], 4)\n",
        "    # })\n",
        "    return {\n",
        "        \"R1\": round(result[\"rouge1\"], 4),\n",
        "        \"R2\": round(result[\"rouge2\"], 4),\n",
        "        \"RL\": round(result[\"rougeL\"], 4),\n",
        "        \"RLsum\": round(result[\"rougeLsum\"], 4),\n",
        "        \"bleu\": round(results_sacrebleu[\"score\"], 4),\n",
        "        \"precision1\":round(results_bert[\"precision\"][0], 4),\n",
        "        \"precision2\":round(results_bert[\"precision\"][1], 4),\n",
        "        \"recall1\":round(results_bert[\"recall\"][0], 4),\n",
        "        \"recall2\":round(results_bert[\"recall\"][1], 4),\n",
        "        \"f1-score1\":round(results_bert[\"f1\"][0], 4),\n",
        "        \"f1-score2\":round(results_bert[\"f1\"][1], 4)\n",
        "    }\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.save_pretrained(output_dir)\n",
        "\n",
        "train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "eval_dataset = load_dataset(eval_file_path, tokenizer)\n",
        "data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          evaluation_strategy = \"epoch\",\n",
        " #         eval_steps = 5000,\n",
        "          learning_rate=1e-5,\n",
        "          save_strategy = \"epoch\",\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          per_device_eval_batch_size=1,\n",
        "          num_train_epochs=num_train_epochs\n",
        "      )\n",
        "\n",
        "trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "          eval_dataset=eval_dataset,\n",
        "          preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
        "          compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}